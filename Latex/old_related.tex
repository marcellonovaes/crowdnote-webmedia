\textbf{RASCUNHO!!!}

\cite{DiSalvo:2013:CAS:2501105.2501113}


* Anotação com CS: \cite{DiSalvo:2013:CAS:2501105.2501113}, a reconstrução de storyline \cite{Kim:2014:JSL:2679600.2680027} e mesmo a composição colaborativa de vídeos \cite{Wilk:2015:VCC:2713168.2713178}.

* Este potencial já foi comprovado em vários trabalhos, como o que foi conduzido por J. A. Redi \cite{Redi2013} com a finalidade de avaliar subjetivamente aspectos estéticos em imagens. Outro trabalho interessante relacionado com o avaliação é o CrowdStudy, um toolkit para avaliação de interfaces Web baseado em Crowdsourcing \cite{Nebeling}.

1.	Qual artefato é gerado?
[2] São geradas thumbnails para vídeos.
[3] Legendas textuais em quadros posicionados em locais específicos no vídeo.
[4] Legendas para videoaulas, geradas a partir de transcrições do áudio.
[5] Transcrição de vídeo aulas. 
[6] Anotações sobre vídeos de vigilância.
[7] Agregaçõs e sumarizações.
[8] Anotações/Contagem sobre itens contidos na cenas (pedestres, ciclistas e veiculos).
[9] Legendas.
[10] Versão antiga do [2]
[11] Tags/Anotaçõs sobre vídeos.
[12] Validar hiperlinks no vídeo.
[13] Anotações de eventos em vídeos esportivos. 

2.	Qual é o objetivo?
[1] Produção colaborativa de documentários, montando storyboards a partir de vídeos produzidos.
[2] Gerar thumbnails que façam sentido para o usuário de forma que possam ser utilizadas para representar sugestões de vídeos e resultados de busca.
[3] Melhorar os vídeos (videoaulas), substituindo textos originais por versões digitais, permitindo uma melhor visualização, assim como adequação, tradução etc.
[4] Enriquecer vídeo aulas com a inserção de legendas geradas a partir da transcrição do áudio.
[5] Enriquecer vídeo aulas com legendas, permitindo inclusive indexação pelas legendas.
[6] Detectar eventos em vídeos de vigilância em tempo viável e com boa acurácia. 
[7] Gerar visualizações baseadas em agregação e sumarização.
[8] Verificar se os workes identificariam/contariam os mesmos itens nas cenas.
[9] Enriquecer os vídeos com legendas.
[11] Gerar índices e categorias (e folksonomia).
[12] Checar se foi alcançado um objetivo anterior: Facilitar a navegação (acessibilidade).
[13] Gerar um dataset anotado.
[14] Produção colaborativa de vídeos.

3.	Como foi definida a cobertura?
[2] Foram definidas questões que deveriam ser respondidas pelo usuário, de forma que fosse necessário navegar pelo vídeos para encontrar as respostas.
[3] Foram mapeados os textos que deveriam ser substituídos.
[4] O objetivo é transcrever toda a fala do professor durante a vídeo aula.
[5] Todo o áudio foi dividido em chunks, o escopo era transcrever todo o áudio.
[6] O escopo compreende todos os vídeos gerados pelas câmeras de vigilância. 
[7] Segmento de vídeo inteiro.
[8] Selecionaram uma coleção de imagens, e todas elas deveriam ser anotadas.
[9] Definiram segmentos de vídeos que deveriam ser legendados na integra.
[12] 

4.	Qual o critério de convergência?
[2] Não houve, eles determinaram inicialmente a quantidade de contribuições que seriam coletadas.
[3] Os quadros foram enviadas para os usuários, com numero de contribuições previamente determinado.
[4] Todas as sentenças são marcadas como Completed.
[5] Quando todos os chunks entram no estado Checked.
[6] Não existe um critério de convergência, trata-se de um trabalho continuo. 
[7] Redundância e filtro de conflitos.
[8] Foi estabelecido inicialmente um numero fixo de contribuições esperado.
[9] Os workers recebem suas respectivas tasks, e o trabalho termina quando todas as tasks são entregues.

5.	Como é modelada a microtask? Como é o processo de colaboração?
[2] O usuário precisava navegar pelo vídeo para responder algumas questões. Os eventos de play, pause e seek do player eram detectados durante a navegação.
[3] O usuário assiste ao vídeo, e ao encontrar um texto manuscrito, seleciona a área e digita o mesmo texto. Neste momento pode inclusive criar traduções para este texto.
[4] Os workers recebem blocos de 5 sentenças de áudio ou texto para editar. Ao iniciar uma edição ele marca como bloqueada a sentença para evitar controle de concorrência.
[5] Workers transcrevem chunks de áudio.
[6] o worker pausa o vídeo quando detectar algum evento e marca os itens que aparecem na cena. (roubo, bagagem abandonada, assalto etc).
[7] O usuário recebe um vídeo, e conforme assiste são apresentadas questões sobre comportamento de pessoas que aparecem nele.
[8] As imagens são apresentadas aos workers que contam quantos pedestres, veículos e ciclistas existe em cada uma delas.
[9] Existem 2 tipos, no primeiro o worker escuta e transcreve em texto, na segunda o ASR sugere o texto e o worker edita/revisa.
[13] Cada task envolve anotar 5 seg de vídeo utilizando uma ferramenta web.


6.	Como as contribuições são validadas?
[2] Não são validadas, as contribuições discrepantes acabam não influenciando significativamente nos máximos locais.
[3] Os usuários, além de adicionar os textos, também vão revisando os textos já adicionados.
[4] Os usuários realizam revisões sobre contribuições já registradas.
[5] Matches (2 workers submetem a mesma transcrição para um chunk de audio) OU confirmação do owner.
[6] Não existe preocupação com falsos positivos, apenas com falsos negativos.
[7] Filtro de conflitos.
[8] Combinação dos resultados e comparação com o output de uma RNA.
[9] As contribuições são aceitas quando pelo menos 2 workers concordam, e no modo Type os textos digitados são comparados com o ASR.



7.	Como é gerado o artefato final?
[2] As contribuições são contabilizadas e são determinados por uma heurística baseada nos máximos locais das séries de replay. 
[3] A versão enriquecida é composta do vídeos original com as delimitações de onde aparecem os textos e apresentados os textos alternativos nestas áreas.
[4] As versões finais das legendas são apresentadas de forma coerente com a vídeo aula. 
[5] Os chunks selecionados compõe a transcrição final.
[6] Reunindo todas as anotações geradas.
[7] Reundância de respostas semelhantes.
[8] Um relatório comparativo de numero de veículos, pedestre e ciclistas em anos diferentes.
[9] As contribuições são alinhadas para formar a legenda final.


8.	Como é validado o artefato final?
[2] Validação feita pelo owner.
[3] Foi feito um questionário de avaliação para usuários onde avaliavam a qualidade do vídeo original e do vídeo enriquecido, sendo possível comparar e determinar quando houve melhoria.
[4] Foi realizado um survey entre os usuários.
[5] Medida de acurácia.
[6] Medida de falsos negativos.
[7] Owner/Gold.
[8] Não existe um artefato final composto pelos parciais, é gerado um relatório. 
[9] Não fica claro como validam o artefato final, acho que só validam as contribuições.



