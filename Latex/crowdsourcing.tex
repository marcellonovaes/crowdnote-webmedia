

Francis Galton wrote in 1907 an article in which he reported an experiment that a crowd of people at an agricultural fair tried to guess the weight of a particular ox. Galton verified that the average of the assumed weights converged to a value very close to the actual weight of the ox and analyzing the distribution of values grounded the Wisdom of the Crowd concept, according to which a heterogeneous crowd large enough tends to provide such a good expert result \cite{GALTON1907}.
 
 
Crowdsourcing is a collaborative approach that uses the Wisdom of the Crowd concept to deliver good quality results using contributions from a crowd of collaborators, being able to distribute, collect, validate and merge large amounts of contributions \cite{Hong:2011:GCR:2018966.2018970,Haas:2015:AMC:2824032.2824062,Mo:2013:OPH:2505515.2505755}. Since this approach is designed to handle a huge number of collaborators and contributions for tasks that require human intelligence \cite{Howe2006}, Crowdsourcing is appropriate to allow the Human Computing paradigm to be applied in a massive-scale online collaboration \cite{TEDMassive} scenario by increasing the performance of the system paralleling tasks \cite{Rohwer:2010:NHC:1837885.1837897}, and improving accuracy according to the Wisdom of Crowd concept.
 
Therefore, human computation is a paradigm that allows us to analyze a problem and identify which tasks really require human intelligence, and Crowdsourcing is an approach that makes it possible for Human Computation systems to handle massive-scale online collaboration. 

The Crowdsourcing approach is supported by four pillars:  The Crowdsourced Task, The Crowdsourcer, The Crowd, and The Crowdsourcing Plataform \cite{6861072}.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.15]{figure/roles}
\caption{Roles - ALTERAR FIGURA!!!}
\label{Roles}
\end{figure}

The Crowdsourced Task is the HIT designed, according to the Human Computing paradigm, to acquire workers' contributions. Instances are generated of the task that are presented to the workers as jobs that must be performed \cite{Difallah:2015:DMC:2736277.2741685}.

The Crowdsourcer is the owner of a project, it may be an individual or institution that wishes to have a completed task. The owner is responsible for starting the crowdsourcing process, defining what task must be completed and how it should be presented to the workers as jobs \cite{6861072}.

The Crowd is the work force that moves the process once it is composed of all the workers who perform the jobs needed to generate the desired result. Each worker performs their work independently, so that the instances of a task can be executed in parallel, according to the Human Computation paradigm \cite{Rohwer:2010:NHC:1837885.1837897}.

The Crowdsourcing platform is the centralizer of the whole proces, serves as an entry point for both the owner making the tasks available and for the workers to execute them. This kind of environment can be something sophisticated like Amazon Mechanical Turk (AMT) \cite{Difallah:2015:DMC:2736277.2741685}, or really simple systems with screens and forms for data collection such the mobile application used by the Google Crowdsource project \cite{google_cs}. A crowdsourcing environment is necessary, as the tasks must be made available to a potentially large number of workers. The crowdsourcing platform is a key element of support to massive-scale collaboration.

The use of a commercial crowdsourcing platform brings benefits such as not having to worry about management's issues of workers, jobs and contributions such as employee recruitment and collection of contributions as well as facilitating employee payments. There is a list of companies that provide this type of service, such as a CrowdFlower and Amazon Mechanical Turk.

To exemplify how Crowdsourcing adds support for massive-scale online collaborations to human computer systems, one can analyze the collaborative processes involved in Luis von Ahn's three most well-known projects,  reCAPTCHA \cite{Simmons:2010:PLV:1869086.1869102},  ESP Game \cite{Robertson:2009:REG:1520340.1520597}, and  Duolingo \cite{vonAhn:2011:THC}.


ESP Game used a game as an input interface, so players who score points by adding tags that describe the images presented to them are viewed by the platform as workers annotating a base of images \cite{Robertson:2009:REG:1520340.1520597}. CAPTCHA tests are very popular on the internet, being widely used by various applications and sites, reCAPTCHA collects the responses that users provide for these tests, and so the platform also sees these interactions as workers annotating images reCAPTCHA \cite{Simmons:2010:PLV:1869086.1869102}. In the Duolingo users carry out translations while learning and practicing another language, and the platform collects these translations as contributions from the workers \cite{Abaunza:2016:BRW:3012430.3012522}.


It is possible to observe in these examples, that in addition the modeling according to the Human Computation paradigm, present a viable way of distributing jobs and collecting results for a potentially huge crowd of collaborators so that these contributions can be used to complete a task.




